[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Maroney’s Site",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   <dbl> 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  <int> 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop <int> 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     <dbl> 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %>%\n  mutate(predicted_murder = predict(dt, USArrests)) %>%\n  select(Murder, predicted_murder) %>%\n  cor() -> corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 2\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html",
    "href": "posts/Problem Set 2 /Index.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Before getting started, we’ll need to make sure the necessary packages installed and libraries loaded. Look at the list below and install if necessary before loading them."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-1",
    "href": "posts/Problem Set 2 /Index.html#step-1",
    "title": "Problem Set 2",
    "section": "Step 1",
    "text": "Step 1\nIn this step we are seeking some understanding of the data we have obtained. Remember this is different from understanding the data we NEED to obtain to best answer a business question. We need to understand both and the differences between the two. But, because we have this bikes_ps.csv data set, we will take a dive into that. We first just get an idea for the dimensions and contents.\n\n# Read in the data set and use glimpse to get an idea.\nbikes = read_csv(\"bikes_ps.csv\")\nglimpse(bikes)\n\nRows: 731\nColumns: 10\n$ date        <date> 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     <dbl> 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     <dbl> 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature <dbl> 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    <dbl> 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    <dbl> 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   <dbl> 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     <dbl> 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…\n\n\nMy analysis: For this data set, there are 731 observations, or rows, each corresponding to a given date in 2011 or 2012. There are 10 columns, or variables, each a different feature about the given day (season, day of the week, temperature, windspeed, etc). Season, Holiday, Weekday, Weather, and Rentals are all numeric variables expressed as integers. Temperature, Realfeel, Humidity, and Windspeed are all numeric variables containing decimals. Date is expressed as a date.\nNow these are the default choices made by the structure of the data set along with the processing intelligence of the read_csv() import function. But our human understanding of the data and its use in solving a business problem are crucial to understanding what the datatype should be and whether changes will need to be made."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-2",
    "href": "posts/Problem Set 2 /Index.html#step-2",
    "title": "Problem Set 2",
    "section": "Step 2",
    "text": "Step 2\nLooking at the data we see that the numeric data types are not truly appropriate for some of the variables. Lets start with some obvious ones like season and holiday. First we need to remember that all variables/features are encoded information. And we need to discern what the original information to be encoded was and how the encoding scheme we see relates to it. For season we can be confident it was meant to indicate the general season in which a rental took place, such as winter, spring, summer, and fall. Instead of using text, these seasons were encoded, or represented by, a number. Now any numbers could be chosen, but typically the numbers might start at the beginning of a year and progress from there, in other words maybe winter = 1, spring = 2, etc. However, we cannot be truly certain without checking. First the best idea is to look at a codebook if one is provided. A codebook is a description of encoding schemes given by the person or persons who actually did the encoding. An alternative method in this case would be to look at the data column along with the season to see whether 1 corresponds to winter months etc. For brevity, we won’t do this for all features, but we’ll take a look at what it means to sleuth out these problems.\n\n# Create a new feature representing the month of year\n# i.e., jan = 1, feb = 2, ..., dec = 12.\n# Then we'll create a table showing season by month\nbikes %>%\n  mutate(month = month(date)) %>%\n  group_by(month) %>%\n  select(season, month) %>%\n  table()\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the above table it becomes clear that the season variable not easily dividable into months. For example season 1 does correspond to wintery months such as December, January, Februrary, and March. But March also has some season 2. Similarly, December has a lot more observations in season 4 (maybe Fall?) than season 1 - Winter. This may suggest that the variable indicates the first official day of winter on December 21 and the first official day of spring on March 21, etc.\nHowever it is encoded, the season feature is not truly numeric. Instead a number on a football jersey, the value is nominal and meant to be an identifier - identifying to which season a day belongs. This is called nominal or categorical data. In R, this is most commonly coded as the factor datatype.\nOther features also use numbers this way and should be represented as factors instead: holiday, weekday, and weather. We can now convert these to factors, and even specify new labels if we’d like.\n\nbikes = bikes %>%\n  mutate_at(vars(season, holiday, weekday, weather), factor) %>%\n  mutate(season = fct_recode(season, \"Winter\"=\"1\", \n                                     \"Spring\"=\"2\",\n                                     \"Summer\"=\"3\",\n                                     \"Fall\"=\"4\"))\n\nMy analysis: Many of the variables measured in this data set, such as season, holiday, weekday, and weather, are integer (still numeric) values in the data, i.e. whole, non-decimal numbers. However, they don’t truly represent numbers - the numbers are assigned to a certain condition within the variable, such as the day of the week or season (e.g. Monday=1, Tuesday=2… or Winter=1, Spring=2…). This makes them prime candidates for transformation into factor variables so that they are easier to work with.\n\nOther Factor Feature Explanations\n\nholiday: This is a binary indicator. A “1” indicates the data is considered a holiday and a 0 that it isn’t. This is categorical and so it needed to be converted to two groups.\nweekday: Here each number represents a day of the week like Sunday, Monday, etc. This means the numbers don’t act as numbers - but instead indicate the day of the week a rental occurs. So we convert it to the categorical factor data type. We could easily change the labels to reflect the day of the week.\nweather: This appears to take on values 1, 2, and 3. But what does it mean? Without a codebook this one is a problem. We cannot be sure whether this is in fact categorical or numerical, and we wouldn’t know that the categories are. Likely, it refers to weather severity or precipitation. For example, perhaps 1 is clear skies no precipitation, 2 is cloudy/rainy, and 3 is stormy. But we would need to reach out to the data creator to be sure."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-3",
    "href": "posts/Problem Set 2 /Index.html#step-3",
    "title": "Problem Set 2",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. Do make things easier to read, we’ll divide summaries by numeric and factor data types.\n\nbikes %>%\n  select(-date) %>%\n  keep(is.numeric) %>%\n  summary()\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\n\nbikes %>%\n  select(-date) %>%\n  keep(is.factor) %>%\n  summary()\n\n    season    holiday weekday weather\n Winter:181   0:710   0:105   1:463  \n Spring:184   1: 21   1:105   2:247  \n Summer:188           2:104   3: 21  \n Fall  :178           3:104          \n                      4:104          \n                      5:104          \n                      6:105          \n\n\nWe see that we have no missing values for factor variables, and are only missing values for the realfeel variable in the set of numeric variables. We are missing 27 values. We could throw these out, but one problem with that is what if they are not missing by random? In other words, what if there are certain days, say when rentals are really high or low that causes this number not to be recorded? Also, although 27 observations are missing realfeel, they are not missing other values. By discarding them, we also throw out all the other information those observations contain. An alternative is to impute the missing values. This means we fill in numbers in the blank spots. But what numbers? We’re essentially making up data by trying to guess what was supposed to be recorded there. If we’re going to do this, we should first try to do no harm. Essentially, we should hope that the statistical properties of the data are not altered or biased by our choice of value. There are number of ways to do this, but for this assignment you’re asked to do the median value imputation. For illustration purposes I’m going to create a copy to compare (you don’t need to do this).\n\nbikes = bikes %>%\n  mutate(realfeel_orig = realfeel)\n\nNow, lets impute the missing values and compare.\n\nbikes = bikes %>%\n  mutate(realfeel = ifelse(is.na(realfeel),\n                           median(realfeel, na.rm = TRUE),\n                           realfeel))\n\nThe above code uses ifelse logic to replace values. It asks a question (checks a condition) and then does different actions based on the answer.\nIs realfeel missing? (is.na(realfeel)):\n\nYES (TRUE): replace with median(reelfeel, na.rm = TRUE).\nNO (FALSE): replace with realfeel (which leaves it unchanged, since we’re just replacing it with itself).\n\nNow we can compare the resulting distributions.\n\nbikes %>%\n  select(realfeel, realfeel_orig) %>%\n  summary()\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and is the same median and basically the same mean. Extreme points are not changed, although the 1st and 3rd quartiles changed a smidgen.\n\n# Remove the copy of original realfeel\nbikes = bikes %>% select(-realfeel_orig)\n\nStep 4\nNow we need to gain some understanding of what we’re trying to predict, rentals. This involves understanding what the variable is and its distribution. Rentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a picture. Additionally, we can look at a picture of rentals over time to see if there is some trending.\n\nbikes %>% select(rentals) %>% summary()\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the max a whopping 8,714 rentals! Across the data the mean is roughly 4500 rentals and the median is only a little higher suggesting that there shouldn’t be an extreme skew and it’s fairly symmetric.\n\nbikes %>%\n  ggplot(aes(x=rentals)) + \n  geom_histogram(aes(y=after_stat(density)),\n                 fill = \"aquamarine\",\n                 color = \"aquamarine3\",\n                 alpha = 0.7) +\n  geom_density(color = \"black\") +\n  labs(title = \"Distribution of Daily Bike Rentals\",\n       x = \"Rentals (count)\") +\n  theme_clean()\n\n\n\n\nFortunately, we don’t seem to have a huge number of outliers and the distribution is not highly skewed. This means that we might not need to make a log-transformation of this feature to make it more normal. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different over-lapping normal distributions. A low, middle, and high one.\nMy analysis: The distribution of rentals is not skewed strongly to the left or right. Transformation of the data is thus not necessary. The data also has three humps, two smaller ones centered around ~2000 rentals and ~7300 rentals respectively, and the largest in the center of the data at ~4600 rentals. These are known as tri-modal distributions. This indicates a possibility that rental days could be sorted into low, middle, and high buckets, based on variables like time of week, temperature, and wind. There do not appear to be a substantial number of outliers."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-5",
    "href": "posts/Problem Set 2 /Index.html#step-5",
    "title": "Problem Set 2",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\nbikes %>%\n  keep(is.numeric) %>%\n  ggpairs()\n\n\n\n\nFirst off we can see that temperature and realfeel have an almost perfectly linear relationship. The correlation is 0.96! This is a suspiciously strong relationship. In fact, this usually means that one variable is a function of the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nMy analysis: realfeel will be excluded. It is near perfectly correlated with temperature, since temperature is based on it. We could disregard temperature and the other variables here which contribute to how it really feels outside, i.e. humidity, windspeed, etc, or we could eliminate the use of realfeel. Eliminating the use of realfeel is useful because the other variables, like humidity and wind speed, tell us important things about the conditions for biking that realfeel doesn’t. One day could be 65 degrees with no wind and no humidity, leading to a realfeel of 65, or it could be 80 degrees with such substantial wind that there is a wind chill of -15, leading to a realfeel of 65 degrees even with no humidity, and a much worse day for biking. Thus, it is better to eliminate realfeel than temperature and all the variables used in concert with it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals. Warmer temps are associated with more rentals (not surprising). But eventually, warmer temperatures result in weather that is too hot for comfort - leading to decreased rentals.\nWe can also check these correlations with corrplot.\n\nbikes %>%\n  keep(is.numeric) %>%\n  cor() %>%\n  corrplot()\n\n\n\n\nSometimes we need to convert features to achieve different objectives.\n\nWe might transform a feature to make it easier for our learning algorithm to use, or\nwe might transform a feature to put it on the same or similar scale with the the other features.\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\nbikes = bikes %>%\n  mutate(temperature = (temperature - mean(temperature))/sd(temperature))\n\nbikes %>%\n  select(temperature) %>%\n  summary()\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\n\nbikes = bikes %>%\n  mutate(windspeed = (windspeed - min(windspeed))/(max(windspeed)-min(windspeed)))\n\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\n\n# Convert every factor type feature into \n# a collection dummy variables.\nbikes_dummies = dummy(bikes, int = TRUE)\n\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones.\n\nbikes_num = bikes %>% keep(is.numeric)\nbikes = bind_cols(bikes_num, bikes_dummies)"
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-6",
    "href": "posts/Problem Set 2 /Index.html#step-6",
    "title": "Problem Set 2",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\n\nbikes = bikes %>%\n  select(-realfeel) %>%\n  mutate(temperature2 = temperature^2)\n\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n# Separate predictors from target feature.\nrentals = bikes$rentals\npredictors = as.matrix(select(bikes, -rentals))\npredictors = predictors \n  \n\n# estimate model\ncv.model = cv.gamlr(x=predictors, y=rentals)\nplot(cv.model)\n\n\n\n\n\nbetamin = coef(cv.model, select = \"min\")\nbetamin\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     seg98\nintercept      7230.726563\ntemperature     981.069533\nhumidity      -3045.345455\nwindspeed     -1894.081850\nseason_Winter  -734.451838\nseason_Spring  -114.725933\nseason_Summer    16.334457\nseason_Fall     304.253490\nholiday_0       461.960081\nholiday_1         .       \nweekday_0      -280.660838\nweekday_1      -112.214878\nweekday_2       -24.389640\nweekday_3         .       \nweekday_4         .       \nweekday_5         3.456294\nweekday_6        64.987612\nweather_1       257.355107\nweather_2         .       \nweather_3     -1680.583291\ntemperature2   -539.486601\n\n\n\nbikes = bikes %>%\n  mutate(pred = as.numeric(predict(cv.model, predictors)))\n\n\nbikes %>%\n  ggplot(aes(x=rentals, y=pred)) +\n  geom_point()\n\n\n\n\nDiscussion\nDiscuss whether you think the features you have in the data make sense for learning to predict daily bike rentals.\nThe features in my data do make sense for predicting daily bike rentals. Many factors influence a person’s decision to bike versus taking other forms of transportation like public transit or a car. These include the type of trip they are taking (business/commute / pleasure / errands / etc) and the weather and the various considerations there. Our data is spot on here. We have information on whether a given day is a holiday, the weather conditions like temperature, wind speed, and humidity, and the day of the week. Weather conditions are particularly useful as on days with heavy rain, snow, or excessive heat people are likely not to bike for exercise or pleasure. The same is true for days with excessive humidity or heavy winds. The data on day of the week is useful as some individuals may use our bike rental service to commute to work. More individuals are also likely to bike on the weekends for pleasure purposes, when they are off work. We can also use the weather and day of the week data in conjutnion to, for example, project how many rentals we may lose to commuters taking public transit on days with prohibitive weather. Holiday data is useful because some with short distances to travel to family or friends may use our bikes. It’s more likely, though, that this would be useful to project falling rentals as people stay in for the Holidays or move long distances via plane or carpooling.\nDiscuss what is means in this case to train or “fit” a model to the data you prepared.\n“Fitting” a model to the data in this case means finding the outputs for a set of inputs that minimize the sum of squared errors across the whole range of inputs. Error is the actual value minus our predicted value, and is squared so that if the error is negative it is still registered as error. In this case our inputs are the data regarding weather, day of week, humidity, whether it is a holiday, and our other variables. The algorithm learns what values for these variables lead to high levels of rentals and which lead to low levels of rentals. This allows it to create a model through which projected weather, humidity, wind speed, etc values can be input to generate a projected number of rentals as an output. The goal is to minimize the amount of error present, i.e. for the projections to be as close as possible to the observed values.\nDiscuss which preparations you did were required to make the learning algorithm work, and which were not strictly required, but maybe are a good idea.\nCleaning the data was an important step in making the learning algorithm. There was a substantial amount of missing data for the realfeel variable, and, even though it ended up being eliminated for multicollinearity reasons, it was good to feed the algorithm a clean data set. We also converted some variables, which were expressed numerically, to factor data, since they were not actually expressing number concepts but were instead things like season or day of the week which had been assigned to numbers (Winter=1, Spring=2… or Monday=1, Tuesday=2…). As was mentioned earlier, realfeel was eliminated for multicollinearity reasons. It was almost perfectly correlated with temperature, and, as discussed in that section of the assignment, the other variables which contribute to it like humidity and wind speed were more valuable than realfeel. Finally, each categorical variable was converted to a dummy variable because our algorithm cannot handle categorical data."
  }
]