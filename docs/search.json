[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Maroney’s Site",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   <dbl> 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  <int> 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop <int> 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     <dbl> 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %>%\n  mutate(predicted_murder = predict(dt, USArrests)) %>%\n  select(Murder, predicted_murder) %>%\n  cor() -> corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nFinal Project\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMatthew Maroney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Corolla Pricing with LASSO\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProblem Set 2\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html",
    "href": "posts/Problem Set 2 /Index.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Before getting started, we’ll need to make sure the necessary packages installed and libraries loaded. Look at the list below and install if necessary before loading them.\n# Clear everything\nrm(list = ls())\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(GGally)\nlibrary(dummy)\nlibrary(corrplot)\nlibrary(gamlr)\nlibrary(glmnet)"
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-1",
    "href": "posts/Problem Set 2 /Index.html#step-1",
    "title": "Problem Set 2",
    "section": "Step 1",
    "text": "Step 1\nIn this step we are seeking some understanding of the data we have obtained. Remember this is different from understanding the data we NEED to obtain to best answer a business question. We need to understand both and the differences between the two. But, because we have this bikes_ps.csv data set, we will take a dive into that. We first just get an idea for the dimensions and contents.\n\n# Read in the data set and use glimpse to get an idea.\nbikes = read_csv(\"bikes_ps.csv\")\nglimpse(bikes)\n\nRows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…\n\n\nMy analysis: For this data set, there are 731 observations, or rows, each corresponding to a given date in 2011 or 2012. There are 10 columns, or variables, each a different feature about the given day (season, day of the week, temperature, windspeed, etc). Season, Holiday, Weekday, Weather, and Rentals are all numeric variables expressed as integers. Temperature, Realfeel, Humidity, and Windspeed are all numeric variables containing decimals. Date is expressed as a date.\nNow these are the default choices made by the structure of the data set along with the processing intelligence of the read_csv() import function. But our human understanding of the data and its use in solving a business problem are crucial to understanding what the datatype should be and whether changes will need to be made."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-2",
    "href": "posts/Problem Set 2 /Index.html#step-2",
    "title": "Problem Set 2",
    "section": "Step 2",
    "text": "Step 2\nLooking at the data we see that the numeric data types are not truly appropriate for some of the variables. Lets start with some obvious ones like season and holiday. First we need to remember that all variables/features are encoded information. And we need to discern what the original information to be encoded was and how the encoding scheme we see relates to it. For season we can be confident it was meant to indicate the general season in which a rental took place, such as winter, spring, summer, and fall. Instead of using text, these seasons were encoded, or represented by, a number. Now any numbers could be chosen, but typically the numbers might start at the beginning of a year and progress from there, in other words maybe winter = 1, spring = 2, etc. However, we cannot be truly certain without checking. First the best idea is to look at a codebook if one is provided. A codebook is a description of encoding schemes given by the person or persons who actually did the encoding. An alternative method in this case would be to look at the data column along with the season to see whether 1 corresponds to winter months etc. For brevity, we won’t do this for all features, but we’ll take a look at what it means to sleuth out these problems.\n\n# Create a new feature representing the month of year\n# i.e., jan = 1, feb = 2, ..., dec = 12.\n# Then we'll create a table showing season by month\nbikes %&gt;%\n  mutate(month = month(date)) %&gt;%\n  group_by(month) %&gt;%\n  select(season, month) %&gt;%\n  table()\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the above table it becomes clear that the season variable not easily dividable into months. For example season 1 does correspond to wintery months such as December, January, Februrary, and March. But March also has some season 2. Similarly, December has a lot more observations in season 4 (maybe Fall?) than season 1 - Winter. This may suggest that the variable indicates the first official day of winter on December 21 and the first official day of spring on March 21, etc.\nHowever it is encoded, the season feature is not truly numeric. Instead a number on a football jersey, the value is nominal and meant to be an identifier - identifying to which season a day belongs. This is called nominal or categorical data. In R, this is most commonly coded as the factor datatype.\nOther features also use numbers this way and should be represented as factors instead: holiday, weekday, and weather. We can now convert these to factors, and even specify new labels if we’d like.\n\nbikes = bikes %&gt;%\n  mutate_at(vars(season, holiday, weekday, weather), factor) %&gt;%\n  mutate(season = fct_recode(season, \"Winter\"=\"1\", \n                                     \"Spring\"=\"2\",\n                                     \"Summer\"=\"3\",\n                                     \"Fall\"=\"4\"))\n\nMy analysis: Many of the variables measured in this data set, such as season, holiday, weekday, and weather, are integer (still numeric) values in the data, i.e. whole, non-decimal numbers. However, they don’t truly represent numbers - the numbers are assigned to a certain condition within the variable, such as the day of the week or season (e.g. Monday=1, Tuesday=2… or Winter=1, Spring=2…). This makes them prime candidates for transformation into factor variables so that they are easier to work with.\n\nOther Factor Feature Explanations\n\nholiday: This is a binary indicator. A “1” indicates the data is considered a holiday and a 0 that it isn’t. This is categorical and so it needed to be converted to two groups.\nweekday: Here each number represents a day of the week like Sunday, Monday, etc. This means the numbers don’t act as numbers - but instead indicate the day of the week a rental occurs. So we convert it to the categorical factor data type. We could easily change the labels to reflect the day of the week.\nweather: This appears to take on values 1, 2, and 3. But what does it mean? Without a codebook this one is a problem. We cannot be sure whether this is in fact categorical or numerical, and we wouldn’t know that the categories are. Likely, it refers to weather severity or precipitation. For example, perhaps 1 is clear skies no precipitation, 2 is cloudy/rainy, and 3 is stormy. But we would need to reach out to the data creator to be sure."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-3",
    "href": "posts/Problem Set 2 /Index.html#step-3",
    "title": "Problem Set 2",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. Do make things easier to read, we’ll divide summaries by numeric and factor data types.\n\nbikes %&gt;%\n  select(-date) %&gt;%\n  keep(is.numeric) %&gt;%\n  summary()\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\n\nbikes %&gt;%\n  select(-date) %&gt;%\n  keep(is.factor) %&gt;%\n  summary()\n\n    season    holiday weekday weather\n Winter:181   0:710   0:105   1:463  \n Spring:184   1: 21   1:105   2:247  \n Summer:188           2:104   3: 21  \n Fall  :178           3:104          \n                      4:104          \n                      5:104          \n                      6:105          \n\n\nWe see that we have no missing values for factor variables, and are only missing values for the realfeel variable in the set of numeric variables. We are missing 27 values. We could throw these out, but one problem with that is what if they are not missing by random? In other words, what if there are certain days, say when rentals are really high or low that causes this number not to be recorded? Also, although 27 observations are missing realfeel, they are not missing other values. By discarding them, we also throw out all the other information those observations contain. An alternative is to impute the missing values. This means we fill in numbers in the blank spots. But what numbers? We’re essentially making up data by trying to guess what was supposed to be recorded there. If we’re going to do this, we should first try to do no harm. Essentially, we should hope that the statistical properties of the data are not altered or biased by our choice of value. There are number of ways to do this, but for this assignment you’re asked to do the median value imputation. For illustration purposes I’m going to create a copy to compare (you don’t need to do this).\n\nbikes = bikes %&gt;%\n  mutate(realfeel_orig = realfeel)\n\nNow, lets impute the missing values and compare.\n\nbikes = bikes %&gt;%\n  mutate(realfeel = ifelse(is.na(realfeel),\n                           median(realfeel, na.rm = TRUE),\n                           realfeel))\n\nThe above code uses ifelse logic to replace values. It asks a question (checks a condition) and then does different actions based on the answer.\nIs realfeel missing? (is.na(realfeel)):\n\nYES (TRUE): replace with median(reelfeel, na.rm = TRUE).\nNO (FALSE): replace with realfeel (which leaves it unchanged, since we’re just replacing it with itself).\n\nNow we can compare the resulting distributions.\n\nbikes %&gt;%\n  select(realfeel, realfeel_orig) %&gt;%\n  summary()\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and is the same median and basically the same mean. Extreme points are not changed, although the 1st and 3rd quartiles changed a smidgen.\n\n# Remove the copy of original realfeel\nbikes = bikes %&gt;% select(-realfeel_orig)\n\nStep 4\nNow we need to gain some understanding of what we’re trying to predict, rentals. This involves understanding what the variable is and its distribution. Rentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a picture. Additionally, we can look at a picture of rentals over time to see if there is some trending.\n\nbikes %&gt;% select(rentals) %&gt;% summary()\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the max a whopping 8,714 rentals! Across the data the mean is roughly 4500 rentals and the median is only a little higher suggesting that there shouldn’t be an extreme skew and it’s fairly symmetric.\n\nbikes %&gt;%\n  ggplot(aes(x=rentals)) + \n  geom_histogram(aes(y=after_stat(density)),\n                 fill = \"aquamarine\",\n                 color = \"aquamarine3\",\n                 alpha = 0.7) +\n  geom_density(color = \"black\") +\n  labs(title = \"Distribution of Daily Bike Rentals\",\n       x = \"Rentals (count)\") +\n  theme_clean()\n\n\n\n\nFortunately, we don’t seem to have a huge number of outliers and the distribution is not highly skewed. This means that we might not need to make a log-transformation of this feature to make it more normal. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different over-lapping normal distributions. A low, middle, and high one.\nMy analysis: The distribution of rentals is not skewed strongly to the left or right. Transformation of the data is thus not necessary. The data also has three humps, two smaller ones centered around ~2000 rentals and ~7300 rentals respectively, and the largest in the center of the data at ~4600 rentals. These are known as tri-modal distributions. This indicates a possibility that rental days could be sorted into low, middle, and high buckets, based on variables like time of week, temperature, and wind. There do not appear to be a substantial number of outliers."
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-5",
    "href": "posts/Problem Set 2 /Index.html#step-5",
    "title": "Problem Set 2",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\nbikes %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()\n\n\n\n\nFirst off we can see that temperature and realfeel have an almost perfectly linear relationship. The correlation is 0.96! This is a suspiciously strong relationship. In fact, this usually means that one variable is a function of the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nMy analysis: realfeel will be excluded. It is near perfectly correlated with temperature, since temperature is based on it. We could disregard temperature and the other variables here which contribute to how it really feels outside, i.e. humidity, windspeed, etc, or we could eliminate the use of realfeel. Eliminating the use of realfeel is useful because the other variables, like humidity and wind speed, tell us important things about the conditions for biking that realfeel doesn’t. One day could be 65 degrees with no wind and no humidity, leading to a realfeel of 65, or it could be 80 degrees with such substantial wind that there is a wind chill of -15, leading to a realfeel of 65 degrees even with no humidity, and a much worse day for biking. Thus, it is better to eliminate realfeel than temperature and all the variables used in concert with it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals. Warmer temps are associated with more rentals (not surprising). But eventually, warmer temperatures result in weather that is too hot for comfort - leading to decreased rentals.\nWe can also check these correlations with corrplot.\n\nbikes %&gt;%\n  keep(is.numeric) %&gt;%\n  cor() %&gt;%\n  corrplot()\n\n\n\n\nSometimes we need to convert features to achieve different objectives.\n\nWe might transform a feature to make it easier for our learning algorithm to use, or\nwe might transform a feature to put it on the same or similar scale with the the other features.\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\nbikes = bikes %&gt;%\n  mutate(temperature = (temperature - mean(temperature))/sd(temperature))\n\nbikes %&gt;%\n  select(temperature) %&gt;%\n  summary()\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\n\nbikes = bikes %&gt;%\n  mutate(windspeed = (windspeed - min(windspeed))/(max(windspeed)-min(windspeed)))\n\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\n\n# Convert every factor type feature into \n# a collection dummy variables.\nbikes_dummies = dummy(bikes, int = TRUE)\n\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones.\n\nbikes_num = bikes %&gt;% keep(is.numeric)\nbikes = bind_cols(bikes_num, bikes_dummies)"
  },
  {
    "objectID": "posts/Problem Set 2 /Index.html#step-6",
    "href": "posts/Problem Set 2 /Index.html#step-6",
    "title": "Problem Set 2",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\n\nbikes = bikes %&gt;%\n  select(-realfeel) %&gt;%\n  mutate(temperature2 = temperature^2)\n\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n# Separate predictors from target feature.\nrentals = bikes$rentals\npredictors = as.matrix(select(bikes, -rentals))\npredictors = predictors \n  \n\n# estimate model\ncv.model = cv.gamlr(x=predictors, y=rentals)\nplot(cv.model)\n\n\n\n\n\nbetamin = coef(cv.model, select = \"min\")\nbetamin\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      seg75\nintercept      7.019397e+03\ntemperature    9.656860e+02\nhumidity      -2.756301e+03\nwindspeed     -1.736339e+03\nseason_Winter -6.995293e+02\nseason_Spring -2.004090e+01\nseason_Summer  4.943541e-01\nseason_Fall    2.921889e+02\nholiday_0      3.421257e+02\nholiday_1     -1.158098e-09\nweekday_0     -1.962522e+02\nweekday_1     -4.761966e+01\nweekday_2      .           \nweekday_3      .           \nweekday_4      .           \nweekday_5      .           \nweekday_6      2.038627e+01\nweather_1      2.528179e+02\nweather_2      .           \nweather_3     -1.570109e+03\ntemperature2  -4.915953e+02\n\n\n\nbikes = bikes %&gt;%\n  mutate(pred = as.numeric(predict(cv.model, predictors)))\n\n\nbikes %&gt;%\n  ggplot(aes(x=rentals, y=pred)) +\n  geom_point()\n\n\n\n\nDiscussion\nDiscuss whether you think the features you have in the data make sense for learning to predict daily bike rentals.\nThe features in my data do make sense for predicting daily bike rentals. Many factors influence a person’s decision to bike versus taking other forms of transportation like public transit or a car. These include the type of trip they are taking (business/commute / pleasure / errands / etc) and the weather and the various considerations there. Our data is spot on here. We have information on whether a given day is a holiday, the weather conditions like temperature, wind speed, and humidity, and the day of the week. Weather conditions are particularly useful as on days with heavy rain, snow, or excessive heat people are likely not to bike for exercise or pleasure. The same is true for days with excessive humidity or heavy winds. The data on day of the week is useful as some individuals may use our bike rental service to commute to work. More individuals are also likely to bike on the weekends for pleasure purposes, when they are off work. We can also use the weather and day of the week data in conjutnion to, for example, project how many rentals we may lose to commuters taking public transit on days with prohibitive weather. Holiday data is useful because some with short distances to travel to family or friends may use our bikes. It’s more likely, though, that this would be useful to project falling rentals as people stay in for the Holidays or move long distances via plane or carpooling.\nDiscuss what is means in this case to train or “fit” a model to the data you prepared.\n“Fitting” a model to the data in this case means finding the outputs for a set of inputs that minimize the sum of squared errors across the whole range of inputs. Error is the actual value minus our predicted value, and is squared so that if the error is negative it is still registered as error. In this case our inputs are the data regarding weather, day of week, humidity, whether it is a holiday, and our other variables. The algorithm learns what values for these variables lead to high levels of rentals and which lead to low levels of rentals. This allows it to create a model through which projected weather, humidity, wind speed, etc values can be input to generate a projected number of rentals as an output. The goal is to minimize the amount of error present, i.e. for the projections to be as close as possible to the observed values.\nDiscuss which preparations you did were required to make the learning algorithm work, and which were not strictly required, but maybe are a good idea.\nCleaning the data was an important step in making the learning algorithm. There was a substantial amount of missing data for the realfeel variable, and, even though it ended up being eliminated for multicollinearity reasons, it was good to feed the algorithm a clean data set. We also converted some variables, which were expressed numerically, to factor data, since they were not actually expressing number concepts but were instead things like season or day of the week which had been assigned to numbers (Winter=1, Spring=2… or Monday=1, Tuesday=2…). As was mentioned earlier, realfeel was eliminated for multicollinearity reasons. It was almost perfectly correlated with temperature, and, as discussed in that section of the assignment, the other variables which contribute to it like humidity and wind speed were more valuable than realfeel. Finally, each categorical variable was converted to a dummy variable because our algorithm cannot handle categorical data."
  },
  {
    "objectID": "posts/Problem Set 3/Corolla Pricing with LASSO.html",
    "href": "posts/Problem Set 3/Corolla Pricing with LASSO.html",
    "title": "Corolla Pricing with LASSO",
    "section": "",
    "text": "The nonstandard R packages you will need to have installed and load for this document are all contained below.\noptions(repos = list(CRAN=\"http://cran.rstudio.com/\"))\nrm(list = ls())\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/v7/f2_6c0fn63xff0tsh5h3sdyr0000gn/T//RtmpQLAH3z/downloaded_packages\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(lubridate)\nlibrary(caret)\nlibrary(dummy)\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "posts/Problem Set 3/Corolla Pricing with LASSO.html#linear-regression",
    "href": "posts/Problem Set 3/Corolla Pricing with LASSO.html#linear-regression",
    "title": "Corolla Pricing with LASSO",
    "section": "Linear Regression",
    "text": "Linear Regression\nA typical approach to prediction problems involves training a model that minimizes deviance. What is deviance? Well its essentially the pile of errors your model builds up over a sample. And while deviance takes different forms for different types of problems (like regression vs. classification), in regression problems the sum of squared errors (SSE) is commonly use as the deviance of the trained model from the observed outcomes in the data. For example, if we have a sample of 100 observations, then for each observation \\(i\\), we can view the actual outcome, \\(y_i\\). and compare it our model’s predicted outcome \\(\\hat{y_i}\\), leading to an error \\(e_i = y_i - \\hat{y_i}\\) and squared error \\(e_i2 = (y_i - \\hat{y_i})\\). We can then pile up all those 100 \\(e^2\\)s to get a pile of errors. When a model generates a smaller pile of errors on a sample than another, then that model is doing a better job of prediction on that sample.\nFor a linear regression model we have a model that is formed by a simple equation. If we have \\(k\\) predictors then our model can be expressed as,\n\\[\n\\begin{align}\n\\hat{y_i} &= \\hat{\\beta_0} + \\hat{\\beta_1}x_{1i} + \\hat{\\beta_2}x_{2i} + \\cdots + \\hat{\\beta_k}x_{ki} \\nonumber \\\\\n&= \\mathbf{x}_i'\\hat{\\boldsymbol{\\beta}}\n\\end{align}\n\\]\nwhere we have vectors, \\(\\mathbf{x}_i = [x_{1i}, x_{2i}, \\cdots, x_{ki}]\\) and \\(\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_k}]\\). For \\(n\\) observations in a sample we are making predictions for, the deviance (or sum of squared errors - SSE) depends on the specific intercept and slopes we choose, i.e., which \\(\\hat{\\boldsymbol{\\beta}}\\).\n\\[\n\\text{dev}\\left(\\hat{\\boldsymbol{\\beta}}\\right) = \\sum_{i=1}^{n} \\left(y_i - \\mathbf{x}_i \\hat{\\boldsymbol{\\beta}} \\right)^2\n\\]\nWe find the \\(\\hat{\\boldsymbol{\\beta}}\\) for a particular sample by figuring out which values of \\(\\hat{\\boldsymbol{\\beta}}\\) minimize deviance on that sample.\nIn predictive modeling, what we care most about is the model’s performance on new (future) data. When we calculate deviance on the data used for training we call it in-sample deviance. But what we really care about is out-of-sample (OOS) deviance. We approximate this the best we can by using data that wasn’t involved in the training process (i.e., test data, cross-validation error, etc.).\n\n# Read in the data and prep data types.\ncars = read_csv('ToyotaCorolla.csv') %&gt;%\n  select(-Id, -Model, -Mfg_Month, -Cylinders, -Quarterly_Tax) %&gt;%\n  rename(Age = Age_08_04) %&gt;%\n  mutate_at(vars(-one_of(\n    c('Price',\n      'Age',\n      'KM',\n      'HP',\n      'CC',\n      'Weight')\n  )), .funs = factor)\n\nMy analysis: This data set, cars, tracks 87 variables over 1436 observations. It is tracking the price of Toyota Corollas based on a number of factors, so each variable is a different feature of the Corolla and each observation is a different Corolla. There is no missing data in this set so we do not need to make a plan to deal with that.\n\nhist(cars$Price)\n\n\n\n\nMy analysis: The Price variable looks alright for a linear regression. While the distribution does look right skewed, this is expected as not many Corollas would be dirt cheap (on the far left side of the distribution). The data does not have an excessive number of outliers.\n\nfeaturePlot(x = cars$Age, \n            y = cars$Price, \n            plot = \"pairs\",\n            auto.key = list(columns = 3))\n\n\n\n\n\nfeaturePlot(x = cars$KM, \n            y = cars$Price, \n            plot = \"pairs\",\n            auto.key = list(columns = 3))\n\n\n\n\n\nfeaturePlot(x = cars$HP, \n            y = cars$Price, \n            plot = \"pairs\",\n            auto.key = list(columns = 3))\n\n\n\n\n\nfeaturePlot(x = cars$Weight, \n            y = cars$Price, \n            plot = \"pairs\",\n            auto.key = list(columns = 3))\n\n\n\n\n\nfeaturePlot(x = cars$CC, \n            y = cars$Price, \n            plot = \"pairs\",\n            auto.key = list(columns = 3))\n\n\n\n\nMy analysis: Some of these variables do have a relationship with Price. Age and KM have the strongest relationship, both being strongly negative, and age being the stronger of the two. This is likely because age/level of use degrades the value of a car quickly. Additionally, as age increases, there are more new features on newer cars which make them more valuable. This occurs independent of the level of driving a car is put though, making age more reliable than just KM. Weight is another variable I tested which has a strong relationship, this one also negative.\nMy analysis: None of the relationships seem too strongly related. Some are not continuous and thus do not have real distributions but are also thus not too strongly correlated that they must be eliminated. None of the continuous variables appeared to correlate even closely to 1:1 with price, as can be observed in my featurePlots above.\nAs a side note when building models with lots of categorical variables, you’ll want to often inspect how big these ‘groups’ are. For Guarantee_Period, there are several values, but we have periods 13, 18, 20, 24, 28, and 36 which each have 1 or 4 cars. This will create problems for us as models which rely on really rare categories. Especially if those categories are absent in future data.\n\ncars %&gt;%\n  select(Guarantee_Period) %&gt;%\n  table()\n\nGuarantee_Period\n   3    6   12   13   18   20   24   28   36 \n1274   77   73    1    1    1    4    1    4 \n\n\nBased on the above, we will only want to include a few guarantee periods as dummies (3, 6, and 12).\n\n# Convert all factors to dummy vars.\ncar_dum = dummy(cars, int = TRUE)\ncar_num = cars %&gt;%\n  keep(is.numeric)\ncars = bind_cols(car_num, car_dum)\nrm(car_dum, car_num)\n\n\n# Partition the data.\nset.seed(5970)\nsamp = createDataPartition(cars$Price, p = 0.65, list = FALSE)\ntraining = cars[samp, ]\ntesting = cars[-samp, ]\nrm(samp)\n\n\n# For OLS remove one dummy from each categorical var\ntraining_ols = training %&gt;%\n  select(-Mfg_Year_1998,\n         -Fuel_Type_CNG,\n         -Met_Color_0,\n         -Color_Beige,\n         -Automatic_0,\n         -Doors_2,\n         -Gears_3,\n         -Mfr_Guarantee_0,\n         -BOVAG_Guarantee_0,\n         -Guarantee_Period_13,\n         -Guarantee_Period_18,\n         -Guarantee_Period_20,\n         -Guarantee_Period_24,\n         -Guarantee_Period_28,\n         -Guarantee_Period_36,\n         -ABS_0,\n         -Airbag_1_0,\n         -Airbag_2_0,\n         -Airco_0,\n         -Automatic_airco_0,\n         -Boardcomputer_0,\n         -CD_Player_0,\n         -Central_Lock_0,\n         -Powered_Windows_0,\n         -Power_Steering_0,\n         -Radio_0,\n         -Mistlamps_0,\n         -Sport_Model_0,\n         -Backseat_Divider_0,\n         -Metallic_Rim_0,\n         -Radio_cassette_0,\n         -Parking_Assistant_0,\n         -Tow_Bar_0\n         )\n\n\n# Train Ordinary Least Squares\nols1 = lm(Price ~ .,\n          data = training_ols)\ncoef(ols1)\n\n        (Intercept)                 Age                  KM                  HP \n      -1.783214e+02       -3.716281e+01       -1.649201e-02        6.323544e+01 \n                 CC              Weight       Mfg_Year_1999       Mfg_Year_2000 \n      -4.461459e+00        1.095513e+01        6.297947e+02        1.346060e+03 \n      Mfg_Year_2001       Mfg_Year_2002       Mfg_Year_2003       Mfg_Year_2004 \n       1.949429e+03        3.861523e+03        5.279390e+03        7.096542e+03 \n   Fuel_Type_Diesel    Fuel_Type_Petrol         Met_Color_1         Color_Black \n       4.489493e+03        7.269491e+02       -7.086229e+01        4.326831e+02 \n         Color_Blue         Color_Green          Color_Grey           Color_Red \n       3.527251e+02        1.132329e+02        3.178587e+02        2.525654e+02 \n       Color_Silver        Color_Violet         Color_White        Color_Yellow \n       2.079004e+02        3.199664e+02       -4.908328e+02        7.198950e+02 \n        Automatic_1             Doors_3             Doors_4             Doors_5 \n       6.023069e+02       -2.731599e+02       -9.138957e+01       -1.358638e+02 \n            Gears_4             Gears_5             Gears_6     Mfr_Guarantee_1 \n       1.806464e+02        7.349898e+02        5.179635e+02        2.959871e+02 \n  BOVAG_Guarantee_1  Guarantee_Period_3  Guarantee_Period_6 Guarantee_Period_12 \n       6.669424e+02       -1.390247e+03       -6.282641e+02       -4.791944e+02 \n              ABS_1          Airbag_1_1          Airbag_2_1             Airco_1 \n      -1.605311e+02        1.221752e+02        1.064131e+02        2.285296e+02 \n  Automatic_airco_1     Boardcomputer_1         CD_Player_1      Central_Lock_1 \n       1.635383e+03       -1.157608e+02        9.489124e+01        2.772641e+01 \n  Powered_Windows_1    Power_Steering_1             Radio_1         Mistlamps_1 \n       2.456190e+02       -3.703417e+02        6.842723e+02        1.586432e+02 \n      Sport_Model_1  Backseat_Divider_1      Metallic_Rim_1    Radio_cassette_1 \n      -3.947345e+01        2.597163e+02        1.044009e+02       -7.509118e+02 \nParking_Assistant_1           Tow_Bar_1 \n      -6.610268e+02       -2.016919e+01 \n\n\n\n# In-sample Deviance Measures\ncaret::postResample(pred = ols1$fitted.values,\n                    obs = training_ols$Price)\n\n        RMSE     Rsquared          MAE \n1007.4567322    0.9275498  741.6332747 \n\n\n\n# Out-of-sample Deviance Measures\ncaret::postResample(pred = predict(ols1, newdata = testing),\n                    obs = testing$Price)\n\n        RMSE     Rsquared          MAE \n3123.1904298    0.4571756  955.7421177 \n\n\n\nols_rmse_test = as.numeric(caret::postResample(pred = predict(ols1, newdata = testing),\n                    obs = testing$Price)[\"RMSE\"])"
  },
  {
    "objectID": "posts/Problem Set 3/Corolla Pricing with LASSO.html#regularization",
    "href": "posts/Problem Set 3/Corolla Pricing with LASSO.html#regularization",
    "title": "Corolla Pricing with LASSO",
    "section": "Regularization",
    "text": "Regularization\nIn settings where you have lots of features that you might include in your model, you need to be careful to select the best model for predicting future data and avoid overfit. How can we do this?\n\nUse a “recipe” that provides a lot of good array of candidate models.\nThen sort through the pile and select from among the candidates to minimize the error on new data.\n\nBehind the scenes that this means is changing the pile of errors (deviance) we are trying to minimize in a way that will shrink the coefficient (effect) a given feature can have on our model. If its coefficient is zero, for example \\(\\hat{\\beta}_i = 0\\) , then it has no impact. If \\(\\hat{\\beta_i} \\neq 0\\) then the model has some effect and the further from zero a coefficient is, the bigger its impact on the model. Bigger impacts can lead to better predictions when its a true signal, but lead to bigger errors when the relationship is noise!!! So our goal is to let coefficients on real signals grow carefully, while shrinking coefficients on noise to zero. We accomplish this by measuring the size of \\(\\hat{\\boldsymbol{\\beta}}\\) in addition to its effect on deviance. In other words we now want to choose values for \\(\\hat{\\boldsymbol{\\beta}}\\) so as to minmize\n\\[\n\\text{dev}\\left(\\hat{\\boldsymbol{\\beta}}\\right) + \\lambda \\cdot \\text{L1Norm}\\left(\\hat{\\boldsymbol{\\beta}}\\right)\n\\]\nwhere \\(\\text{L1Norm()}\\) is the “size” of our coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) and this term is mathematically determined. The number \\(\\lambda\\) is a penalty or the “price” we charge ourselves for letting \\(\\hat{\\boldsymbol{\\beta}}\\) get too big. But this number is not pre-determined. It is a hyperparameter that we have to choose before doing a minimization problem.\n\nAs \\(\\lambda\\) gets larger we want to shrink the size of \\(\\hat{\\boldsymbol{\\beta}}\\) so that we have fewer nonzero coefficients (making the model more sparse) and the nonzero ones are closer to zero.\nAs \\(\\lambda\\) gets smaller we want to allow the size of \\(\\hat{\\boldsymbol{\\beta}}\\) to grow meaning coefficients get bigger and\n\nAny value of \\(\\lambda\\) we choose will have an impact on the coefficients learned from the data, which affects our predictions. Each value of \\(\\lambda\\) we choose can be referred to as a regularization path. Again if \\(\\lambda\\) is a price, then each price we set gives us a different prediction/error capacity. Instead of choosing just one value and hoping for the best, we can solve the problem and calculate different vector of coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) for each value of \\(\\lambda\\). By default, the glmnet package will calculate coefficients for each of 100 different \\(\\lambda\\) penalty values.\n\nX = as.matrix(select(training, -Price))\ny = training$Price\n\nlasso_fit = glmnet(X, y)\nplot(lasso_fit, \"lambda\")\n\n\n\n\nThe plot above shows the model results over different \\(\\lambda\\) values. Imagine 100 vertical slices (represeneted by x-axis values) for different choices of \\(\\lambda\\). The lines in the graph represent the values (sizes) of coefficients for potential features in our model. At each x-axis value, you can see which features have nonzero coefficients and how big or small some may be. As we go right, the value of \\(\\lambda\\) increases until all coefficients shrink to zero and the model is extremely sparse. Low values of \\(\\lambda\\) are on the left, where most all features have larger, non-zero coefficients and our model is quite large.\nWe can look at the model that results from any one regularization path by choosing a \\(\\lambda\\) value to look at. You’ll see higher values lead to sparse models, lower values lead to complex models.\n\ncoef(lasso_fit, s = exp(2))\n\n87 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)          4.569550e+03\nAge                 -5.502178e+01\nKM                  -1.632840e-02\nHP                   5.296258e+01\nCC                  -3.421634e+00\nWeight               1.168093e+01\nMfg_Year_1998       -8.616763e+02\nMfg_Year_1999       -4.503567e+02\nMfg_Year_2000        .           \nMfg_Year_2001        3.376733e+02\nMfg_Year_2002        2.049235e+03\nMfg_Year_2003        3.230824e+03\nMfg_Year_2004        4.960007e+03\nFuel_Type_CNG       -7.388063e+02\nFuel_Type_Diesel     2.887021e+03\nFuel_Type_Petrol     .           \nMet_Color_0          4.043266e+01\nMet_Color_1         -1.071960e+01\nColor_Beige         -2.650599e+02\nColor_Black          1.285332e+02\nColor_Blue           6.423893e+01\nColor_Green         -1.140338e+02\nColor_Grey           5.470243e+01\nColor_Red           -1.537235e-01\nColor_Silver        -1.500026e+00\nColor_Violet         .           \nColor_White         -7.112599e+02\nColor_Yellow         3.273754e+02\nAutomatic_0         -5.078627e+02\nAutomatic_1          1.587488e-09\nDoors_2              7.085292e+01\nDoors_3             -1.018097e+02\nDoors_4              .           \nDoors_5              .           \nGears_3             -6.610477e+02\nGears_4             -1.743563e+02\nGears_5              7.379914e+01\nGears_6              .           \nMfr_Guarantee_0     -2.865842e+02\nMfr_Guarantee_1      .           \nBOVAG_Guarantee_0   -4.354167e+02\nBOVAG_Guarantee_1    1.631778e+02\nGuarantee_Period_3  -6.391368e+02\nGuarantee_Period_6   .           \nGuarantee_Period_12  1.804401e+02\nGuarantee_Period_13  .           \nGuarantee_Period_18  .           \nGuarantee_Period_20  1.233762e+03\nGuarantee_Period_24  7.821458e+02\nGuarantee_Period_28  1.402703e+03\nGuarantee_Period_36 -4.871054e+02\nABS_0                8.412695e+01\nABS_1               -4.069281e-10\nAirbag_1_0          -1.953724e+01\nAirbag_1_1           .           \nAirbag_2_0          -5.088188e+01\nAirbag_2_1           3.275354e-10\nAirco_0             -2.110072e+02\nAirco_1              .           \nAutomatic_airco_0   -1.668244e+03\nAutomatic_airco_1    .           \nBoardcomputer_0      .           \nBoardcomputer_1      .           \nCD_Player_0         -1.230678e+02\nCD_Player_1          .           \nCentral_Lock_0      -6.151745e+01\nCentral_Lock_1       .           \nPowered_Windows_0   -1.834397e+02\nPowered_Windows_1    .           \nPower_Steering_0     1.981174e+02\nPower_Steering_1     .           \nRadio_0              .           \nRadio_1              .           \nMistlamps_0         -1.452774e+02\nMistlamps_1          2.644046e-10\nSport_Model_0        2.989354e+01\nSport_Model_1       -2.805791e-10\nBackseat_Divider_0  -1.788242e+02\nBackseat_Divider_1   5.401914e-10\nMetallic_Rim_0      -8.248562e+01\nMetallic_Rim_1       .           \nRadio_cassette_0     5.188823e+01\nRadio_cassette_1     .           \nParking_Assistant_0  5.192945e+02\nParking_Assistant_1 -9.986572e-09\nTow_Bar_0            1.649845e+01\nTow_Bar_1           -2.683281e-10\n\n\nWhich path is best for prediction? We can compare performance on the training data vs the testing data over all regularization paths and also compare to our OLS “kitchen sink” model in which we just added everything without regularization.\n\nlogLambda = seq(0, 8, 0.1)\ntesting_mat = as.matrix(testing)\nRMSE_train = c()\nRMSE_test = c()\nfor (i in logLambda) {\n  train_i = as.numeric(postResample(pred=predict(lasso_fit, X, s=exp(i)), obs=y)[\"RMSE\"])\n  RMSE_train = append(RMSE_train, train_i)\n  test_i = postResample(pred=predict(lasso_fit, testing_mat[,2:87], s=exp(i)), obs=testing_mat[,1])[\"RMSE\"]\n  RMSE_test = append(RMSE_test, as.numeric(test_i))\n}\nplot(x = logLambda, y = RMSE_train, type='l', col = \"blue\",\n     main = \"RMSE Deviance over Lambda: Train vs Test\",\n     xlab = \"Log Lambda\",\n     ylab = \"RMSE Deviance\")\nlines(x = logLambda, y = RMSE_test, col=\"red\", lw=2)\nabline(h=ols_rmse_test, col=\"black\")\nlegend(3.25, 2700, legend = c(\"LASSO Training\", \"LASSO Testing\", \"OLS Testing\"), fill=c(\"blue\", \"red\", \"black\"))\n\n\n\n\nCross-validated Choice of \\(\\lambda\\)\nBut what value of \\(\\lambda\\) do we choose to get the best model? From above we’d like to minimize deviance/error on new future data. Our test data gives us an indication of where this might be. We can use cross-validation to check each value of \\(\\lambda\\) and find the one that gives us minimum or near-minimum error on hold-out data. This is our best guess as to performance on new future data.\n\ncv_lasso = cv.glmnet(X, y, type.measure = \"mse\", nfolds=20)\nplot(cv_lasso)\n\n\n\n\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n87 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)          2.039867e+03\nAge                 -9.268330e+01\nKM                  -1.576064e-02\nHP                   1.957395e+01\nCC                   .           \nWeight               1.408500e+01\nMfg_Year_1998        .           \nMfg_Year_1999        .           \nMfg_Year_2000        .           \nMfg_Year_2001        .           \nMfg_Year_2002        1.201695e+03\nMfg_Year_2003        1.850921e+03\nMfg_Year_2004        3.198377e+03\nFuel_Type_CNG       -5.172226e+02\nFuel_Type_Diesel     .           \nFuel_Type_Petrol     .           \nMet_Color_0          .           \nMet_Color_1          .           \nColor_Beige          .           \nColor_Black          .           \nColor_Blue           .           \nColor_Green         -7.267889e+00\nColor_Grey           .           \nColor_Red            .           \nColor_Silver         .           \nColor_Violet         .           \nColor_White         -4.444337e+02\nColor_Yellow         .           \nAutomatic_0         -4.333903e+01\nAutomatic_1          1.852730e-11\nDoors_2              .           \nDoors_3              .           \nDoors_4              .           \nDoors_5              .           \nGears_3              .           \nGears_4              .           \nGears_5              .           \nGears_6              .           \nMfr_Guarantee_0     -2.072237e+02\nMfr_Guarantee_1      .           \nBOVAG_Guarantee_0   -3.708453e+02\nBOVAG_Guarantee_1    4.353313e+01\nGuarantee_Period_3  -2.726044e+02\nGuarantee_Period_6   .           \nGuarantee_Period_12  1.143311e+02\nGuarantee_Period_13  .           \nGuarantee_Period_18  .           \nGuarantee_Period_20  .           \nGuarantee_Period_24  .           \nGuarantee_Period_28  .           \nGuarantee_Period_36  .           \nABS_0                .           \nABS_1                .           \nAirbag_1_0           .           \nAirbag_1_1           .           \nAirbag_2_0           .           \nAirbag_2_1           .           \nAirco_0             -1.220682e+02\nAirco_1              .           \nAutomatic_airco_0   -1.795574e+03\nAutomatic_airco_1    .           \nBoardcomputer_0      .           \nBoardcomputer_1      .           \nCD_Player_0         -6.766796e+01\nCD_Player_1          .           \nCentral_Lock_0      -7.473071e+01\nCentral_Lock_1       .           \nPowered_Windows_0   -8.656770e+01\nPowered_Windows_1    .           \nPower_Steering_0     .           \nPower_Steering_1     .           \nRadio_0              .           \nRadio_1              .           \nMistlamps_0         -1.100573e+02\nMistlamps_1          2.767985e-11\nSport_Model_0        .           \nSport_Model_1        .           \nBackseat_Divider_0   .           \nBackseat_Divider_1   .           \nMetallic_Rim_0      -1.450066e+01\nMetallic_Rim_1       .           \nRadio_cassette_0     .           \nRadio_cassette_1     .           \nParking_Assistant_0  .           \nParking_Assistant_1  .           \nTow_Bar_0            .           \nTow_Bar_1            .           \n\n\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n87 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)          2.747413e+03\nAge                 -1.006359e+02\nKM                  -1.530218e-02\nHP                   1.754265e+01\nCC                   .           \nWeight               1.374312e+01\nMfg_Year_1998        .           \nMfg_Year_1999        .           \nMfg_Year_2000        .           \nMfg_Year_2001        .           \nMfg_Year_2002        6.275309e+02\nMfg_Year_2003        1.161437e+03\nMfg_Year_2004        2.094682e+03\nFuel_Type_CNG        .           \nFuel_Type_Diesel     .           \nFuel_Type_Petrol     .           \nMet_Color_0          .           \nMet_Color_1          .           \nColor_Beige          .           \nColor_Black          .           \nColor_Blue           .           \nColor_Green          .           \nColor_Grey           .           \nColor_Red            .           \nColor_Silver         .           \nColor_Violet         .           \nColor_White          .           \nColor_Yellow         .           \nAutomatic_0          .           \nAutomatic_1          .           \nDoors_2              .           \nDoors_3              .           \nDoors_4              .           \nDoors_5              .           \nGears_3              .           \nGears_4              .           \nGears_5              .           \nGears_6              .           \nMfr_Guarantee_0     -8.108338e+01\nMfr_Guarantee_1      .           \nBOVAG_Guarantee_0   -7.355626e+01\nBOVAG_Guarantee_1    .           \nGuarantee_Period_3   .           \nGuarantee_Period_6   .           \nGuarantee_Period_12  .           \nGuarantee_Period_13  .           \nGuarantee_Period_18  .           \nGuarantee_Period_20  .           \nGuarantee_Period_24  .           \nGuarantee_Period_28  .           \nGuarantee_Period_36  .           \nABS_0                .           \nABS_1                .           \nAirbag_1_0           .           \nAirbag_1_1           .           \nAirbag_2_0           .           \nAirbag_2_1           .           \nAirco_0             -5.937139e+01\nAirco_1              .           \nAutomatic_airco_0   -1.977027e+03\nAutomatic_airco_1    .           \nBoardcomputer_0      .           \nBoardcomputer_1      .           \nCD_Player_0          .           \nCD_Player_1          .           \nCentral_Lock_0      -4.789938e+01\nCentral_Lock_1       .           \nPowered_Windows_0   -9.564883e+01\nPowered_Windows_1    .           \nPower_Steering_0     .           \nPower_Steering_1     .           \nRadio_0              .           \nRadio_1              .           \nMistlamps_0          .           \nMistlamps_1          .           \nSport_Model_0        .           \nSport_Model_1        .           \nBackseat_Divider_0   .           \nBackseat_Divider_1   .           \nMetallic_Rim_0       .           \nMetallic_Rim_1       .           \nRadio_cassette_0     .           \nRadio_cassette_1     .           \nParking_Assistant_0  .           \nParking_Assistant_1  .           \nTow_Bar_0            .           \nTow_Bar_1            .           \n\n\nCompare to OLS\nFinally, lets see what this regularization process has yielded us. We first fit a plain OLS model with all variables, something that might likely lead to overfitting and including lots of noise. Overall prediction was not that great. Lets see how much better we do on the test data with regularization and cross-validation.\n\nols_res = postResample(pred=predict(ols1, testing), obs=testing$Price)\nprint(ols_res)\n\n        RMSE     Rsquared          MAE \n3123.1904298    0.4571756  955.7421177 \n\n\n\ncvlasso_res = postResample(pred=predict(cv_lasso, testing_mat[,2:87], s=\"lambda.min\"),\n             obs=testing_mat[,1])\nprint(cvlasso_res)\n\n        RMSE     Rsquared          MAE \n1144.9031848    0.8868547  851.6820741 \n\n\n\ncvlasso_rmse = as.numeric(cvlasso_res[\"RMSE\"])\npaste0(\"Regularization reduced RMSE on test data by: \", round((ols_rmse_test-cvlasso_rmse)/ols_rmse_test*100,2), \"%\")\n\n[1] \"Regularization reduced RMSE on test data by: 63.34%\"\n\n\nMy analysis: After regularization, my model is effective for predicting the price of Corollas for CorollaCrowd. Identifying and removing variables which caused overfitting in the model reduced RMSE by 63.34% and increased the Rsquared, the extent to which the variables in the model explain the variation in the data, by ~43%. This means our model is much more effective at capturing and using important variables to determine a given Corolla’s value. The model is not perfect - the Rsquared of the regularized model is still only 89%, meaning it does not explain over 10% of the variation in the data. This means, though, that our model is a generally good predictor."
  },
  {
    "objectID": "projects/sentiment analysis of politician tweets /finalproject.html",
    "href": "projects/sentiment analysis of politician tweets /finalproject.html",
    "title": "Final Project",
    "section": "",
    "text": "In this project, I will examine political polarization. Polarization has been an incredibly impactful trend in recent American politics. Both the left and right in this country are becoming more extreme over time. Congress has failed to produce bipartisan cooperation for at least the past two decades. Each party uses the filibuster to hold the other hostage when they are in power. This leaves America falling behind the rest of the world as we are not able to respond effectively to problems the country faces.\nIn this project I will analyze a set of tweets from politicians across the country. I will analyze the sentiment of words in their tweet based on whether the tweet is partisan or nonpartisan. From that, I hope to glean information about the kind of words politicans are using to describe partisan vs. nonpartisan issues. If my intuitions are correct, partisan tweets will use more negative language than nonpartisan tweets. This will backup arguments that political polartization is happening in this country and that bipartisan cooperation is extremely difficult.\nLoading Necessary Packages\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(SnowballC)\n\nReading in data and eliminating unnecessary variables\n\ndata2 = read_csv(\"political_social_media.csv\") %&gt;%\n  select(-\"_golden\", -\"_unit_state\", -\"_trusted_judgments\", -\"_last_judgment_at\", -\"audience:confidence\", -message, -\"message:confidence\", -\"orig__golden\", -\"audience_gold\", -\"bias_gold\", -bioid, -embed, -id, -label, -\"message_gold\", -source, -\"bias:confidence\", -\"_unit_id\")\n\nRows: 5000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): _unit_state, _last_judgment_at, audience, bias, message, bioid, em...\ndbl  (5): _unit_id, _trusted_judgments, audience:confidence, bias:confidence...\nlgl  (5): _golden, orig__golden, audience_gold, bias_gold, message_gold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata3 = read_csv(\"political_social_media.csv\") %&gt;%\n  select(-\"_golden\", -\"_unit_state\", -\"_trusted_judgments\", -\"_last_judgment_at\", -\"audience:confidence\", -\"message:confidence\", -\"orig__golden\", -\"audience_gold\", -\"bias_gold\", -bioid, -embed, -id, -label, -\"message_gold\", -source, -\"bias:confidence\", -\"_unit_id\", -bias, -audience)\n\nRows: 5000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): _unit_state, _last_judgment_at, audience, bias, message, bioid, em...\ndbl  (5): _unit_id, _trusted_judgments, audience:confidence, bias:confidence...\nlgl  (5): _golden, orig__golden, audience_gold, bias_gold, message_gold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata.token = data2 %&gt;%\n  unnest_tokens(word, text, token = \"words\") %&gt;% \n  select(-audience)\n\nHere, I’ve “tokenized” my data. This means I’ve extracted each individual word from all the tweets and categorized them by partisan/nonpartisan based on the original tweet the word came from.\n\ndata.token %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(n = 10)\n\n# A tibble: 10 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the    7728\n 2 to     5955\n 3 and    3693\n 4 of     3404\n 5 in     2940\n 6 a      2534\n 7 http   2161\n 8 for    2142\n 9 on     1783\n10 t.co   1701\n\n\nHere, I’ve found the most common words across tweets. I will go on to remove “stop words,” which are words which are not useful for my analysis, such as overly common words like “the,” “to,” “and,” or “of,” alongside things which are not words which tokenization still gathered up, such as “t.co” or “http.”\n\ndata(\"stop_words\")\ndata.token_nostop = data.token %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\nHere, I’ve removed “stop words.” These are common words (or other things which commonly appear in tweets from politicians, like numbers and years) which aren’t relevant for sentiment analysis. I’ve removed them from my data set here and will use this code many times again throughout this project to remove the same set of stop words from the data.\n\ndata.token_nostop %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(n = 100)\n\n# A tibble: 100 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 house        450\n 2 bill         332\n 3 president    325\n 4 congress     305\n 5 act          303\n 6 time         296\n 7 veterans     288\n 8 health       267\n 9 government   250\n10 jobs         246\n# ℹ 90 more rows\n\n\nThis is the new list of the most frequent words after stop words have been removed.\n\nsent_words = get_sentiments(\"afinn\")\n\n\ntweet.sentiments = data.token_nostop %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  ungroup() %&gt;%\n  distinct()\n\nHere, I’ve brought in “sent_words,” this is a large set of words each with a sentiment score assigned to it based on its positivity or negativity.\nNext, I’ve assigned each word a sentiment score based on what’s in the “sent_words” data set.\nPartisan Tweet Sentiments - Bias Variable\n\npartisan.sentiments = tweet.sentiments %&gt;%\n  filter(bias==\"partisan\")\n\n\nmean(partisan.sentiments$value)\n\n[1] -0.2353896\n\n\n\npartisan.value = partisan.sentiments$value \nhist(partisan.value)\n\n\n\n\nThe mean sentiment value for any given word in tweets marked as “partisan” is -0.24. As you can see from the histogram of sentiment values of words in partisan tweets, there are many more negative words than positive words. This indicates politicians use more charged and negative language when discussing partisan issues.\nNeutral Tweet Sentiments - Bias Variable\n\nneutral.sentiments = tweet.sentiments %&gt;%\n  filter(bias==\"neutral\")\n\n\nmean(neutral.sentiments$value)\n\n[1] 0.02155689\n\n\n\nneutral.value = neutral.sentiments$value\nhist(neutral.value)\n\n\n\n\nThe mean sentiment for any given word in tweets marked as “neutral” is 0.02, and looking at the histogram there is not that much of a difference in the frequency of positive and negative words. While negative words have a higher peak between -2 and -3, words of low positive sentiment are present more often than words of low negative sentiment, balancing the distribution out.\nOverall, we can see that words in partisan tweets are typically more negative. This reflects and backs up the trend of political polarization and the collapse of bipartisanship discussed in the exposition.\nTokenization and Sentiment Gathering - Audience Variable\n\ndata.token2 = data2 %&gt;%\n  unnest_tokens(word, text, token = \"words\") %&gt;% \n  select(-bias)\n\n\ndata(\"stop_words\")\ndata.token_nostop2 = data.token2 %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\n\ntweet.sentiments2 = data.token_nostop2 %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  ungroup() %&gt;%\n  distinct()\n\nSentiment of Tweets aimed at the Nation - Audience Variable\n\nnational.sentiments = tweet.sentiments2 %&gt;%\n  filter(audience==\"national\")\n\n\nmean(national.sentiments$value)\n\n[1] -0.1317747\n\n\n\nnational.value=national.sentiments$value\nhist(national.value)\n\n\n\n\nMean sentiment for words in tweets aimed at the national constituency is -0.13. This is backed up by the histogram which shows words with negative sentiment are much more common in these tweets.\nSentiment of Tweets aimed at a politician’s local constituency - Audience Variable\n\ncons.sentiments = tweet.sentiments2 %&gt;%\n  filter(audience==\"constituency\")\n\n\nmean(cons.sentiments$value)\n\n[1] 0.3592677\n\n\n\ncons.value=cons.sentiments$value\nhist(cons.value)\n\n\n\n\nMean sentiment value of words in tweets aimed at a politician’s local constituency is 0.36. This is reflected in the histogram which shows an intense concentration of positive words in these tweets. This makes sense, as politicians would not discuss their own constituency or local area in a negative manner as they rely on those people for re-election\nThe difference in mean sentiment of words in tweets aimed at the nation versus a politician’s local constituency also backs up the trends discussed up top. While politicians put on a happy face for their voters, they discuss national politics negatively.\nTokenization and Sentiment Gathering - Message Variable\n\ndata.token3 = data3 %&gt;%\n  unnest_tokens(word, text, token = \"words\")\n\n\ndata(\"stop_words\")\ndata.token_nostop3 = data.token3 %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\n\ntweet.sentiments3 = data.token_nostop3 %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  ungroup() %&gt;%\n  distinct()\n\nSentiment of Tweets with a Policy Focused Message - Message Variable\n\npolicy.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"policy\")\n\n\nmean(policy.sentiments$value)\n\n[1] -0.08345979\n\n\n\npolicy.value=policy.sentiments$value\nhist(policy.value)\n\n\n\n\nMean sentiment of words in tweets discussing policy is -0.08. Tweets focused on policy are largely a wash - politicians tweet negatively about policies they oppose, and positively about those they support.\nSentiment of Tweets with an Attack Message - Message Variable\n\nattack.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"attack\")\n\n\nmean(attack.sentiments$value)\n\n[1] -0.5368421\n\n\n\nattack.value=attack.sentiments$value\nhist(attack.value)\n\n\n\n\nMean sentiment of words used in tweets with a message meant to attack is -0.53. This makes sense - when attacking another politician, party, or policy, using negative language is necessary. This is reflected in the histogram - these tweets have an extremely high frequency of quite negative words.\nSentiment of Tweets with a Support Message - Message Variable\n\nsupp.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"support\")\n\n\nmean(supp.sentiments$value)\n\n[1] 0.2173038\n\n\n\nsupp.value=supp.sentiments$value\nhist(supp.value)\n\n\n\n\nTweets with a message meant to support have words with a mean sentiment value of 0.22. This makes sense - supportive messages need positive words. This is also backed up by the histogram - these tweets have high frequency of decently positive words.\nSentiment of Tweets with a message focused on Informing - Message Variable\n\ninfo.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"information\")\n\n\nmean(info.sentiments$value)\n\n[1] 0.07552083\n\n\n\ninfo.value=info.sentiments$value\nhist(info.value)\n\n\n\n\nTweets meant to inform have words with a mean sentiment value of 0.07. This variable being a wash is intuitive. Politicians have to inform their followers of both positive and negative events, and it makes sense they would use language in line with that sentiment in both cases. This is reflected in the similar frequency of positive and negative words displayed in the histogram.\nSentiment of Tweets with a Personal Message - Message Variable\n\npersonal.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"personal\")\n\n\nmean(personal.sentiments$value)\n\n[1] 0.4351464\n\n\n\npersonal.value=personal.sentiments$value\nhist(personal.value)\n\n\n\n\nWords in tweets with a personal message have a mean sentiment value of 0.44. This is also intuitive. When politicians share personal messages or information, it is most often to build up their own brand. It makes sense they would use more positive language to do this, and that’s backed up by the histogram which shows high intensity of positive words in these tweets.\nSentiment of Tweets focused on Voter Mobilization - Message Variable\n\nmobil.sentiments = tweet.sentiments3 %&gt;%\n  filter(message==\"mobilization\")\n\n\nmean(mobil.sentiments$value)\n\n[1] 0.5044248\n\n\n\nmobil.value=mobil.sentiments$value\nhist(mobil.value)\n\n\n\n\nWords in tweets meant to mobilize voters have an average sentiment of 0.5. When politicians want to mobilize their followers, they share positive messages about things like the possibility of good change in the world. Few people are encouraged by messages of hate. This is reinforced by the histogram which shows mostly positive words in these tweets.\nSentiment of Tweets with a message focused on the politician’s constituency - Message Variable\n\ncons.sentiments.msg = tweet.sentiments3 %&gt;%\n  filter(message==\"constituency\")\n\n\nmean(cons.sentiments.msg$value)\n\n[1] 1.156522\n\n\n\ncons.msg.value=cons.sentiments.msg$value\nhist(cons.msg.value)\n\n\n\n\nMean sentiment of words in constituency-focused tweets is 1.16. This is the highest average sentiment value we’ve seen yet. This also makes sense. When politicians are sharing messages about their constituency, even in negative circumstances, they tend to use positive language. They want to describe their constituents in a positive light at almost all times, and any negative language used in these tweets is likely not describing the constituents themselves. This is visible in the histogram which shows an extremely skewed distribution heavily favoring positive words.\nWhole Tweet Sentiment Analysis - Message Variable\n\ndata4 = read_csv(\"political_social_media.csv\") %&gt;%\n  select(-\"_golden\", -\"_unit_state\", -\"_trusted_judgments\", -\"_last_judgment_at\", -\"audience:confidence\", -\"message:confidence\", -\"orig__golden\", -\"audience_gold\", -\"bias_gold\", -bioid, -embed, -label, -\"message_gold\", -source, -\"bias:confidence\", -\"_unit_id\", -bias, -audience)\n\nRows: 5000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): _unit_state, _last_judgment_at, audience, bias, message, bioid, em...\ndbl  (5): _unit_id, _trusted_judgments, audience:confidence, bias:confidence...\nlgl  (5): _golden, orig__golden, audience_gold, bias_gold, message_gold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata.token4 = data4 %&gt;%\n  unnest_tokens(word, text, token = \"words\")\n\ndata.token_nostop4 = data.token4 %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\ntweet.sentiments4 = data.token_nostop4 %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  group_by(id) %&gt;%\n  mutate(sentiment = mean(value)) %&gt;%\n  ungroup() %&gt;%\n  select(-value, -word) %&gt;%\n  distinct()\n\n\nggplot(tweet.sentiments4, aes(x=sentiment, fill=message)) +\n  geom_density(alpha=.25) +\n  theme_economist_white()\n\n\n\n\nHere, I analyzed the average sentiment of a given tweet based on all the relevant words in it. The accompanying graph then shows the distribution of tweet sentiment across different message focuses. As is visible in the graph, tweets with a message focused on constituency are the most positive, followed by tweets with a personal message. Tweets with a message focused on attacking are by far the most negative.\nWhole Tweet Sentiment Analysis - Bias Variable\n\ndata5 = read_csv(\"political_social_media.csv\") %&gt;%\n  select(-\"_golden\", -\"_unit_state\", -\"_trusted_judgments\", -\"_last_judgment_at\", -\"audience:confidence\", -message, -\"message:confidence\", -\"orig__golden\", -\"audience_gold\", -\"bias_gold\", -bioid, -embed, -label, -\"message_gold\", -source, -\"bias:confidence\", -\"_unit_id\", -audience)\n\nRows: 5000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): _unit_state, _last_judgment_at, audience, bias, message, bioid, em...\ndbl  (5): _unit_id, _trusted_judgments, audience:confidence, bias:confidence...\nlgl  (5): _golden, orig__golden, audience_gold, bias_gold, message_gold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata.token5 = data5 %&gt;%\n  unnest_tokens(word, text, token = \"words\")\n\ndata.token_nostop5 = data.token5 %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\ntweet.sentiments5 = data.token_nostop5 %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  group_by(id) %&gt;%\n  mutate(sentiment = mean(value)) %&gt;%\n  ungroup() %&gt;%\n  select(-value, -word) %&gt;%\n  distinct()\n\n\nggplot(tweet.sentiments5, aes(x=sentiment, fill=bias)) +\n  geom_density(alpha=.25) +\n  theme_economist_white()\n\n\n\n\nHere, I analyzed the sentiment of entire tweets based on the positivity/negativity of each word in them split by partisan or neutral bias. Partisan tweets are more negative, while neutral tweets are more biased. This backs up the trend of political polarization and the trend of moving away from bipartisanship discussed earlier.\nWhole Tweet Sentiment Analysis - Audience Variable\n\ndata6 = read_csv(\"political_social_media.csv\") %&gt;%\n  select(-\"_golden\", -\"_unit_state\", -\"_trusted_judgments\", -\"_last_judgment_at\", -\"audience:confidence\", -message, -\"message:confidence\", -\"orig__golden\", -\"audience_gold\", -\"bias_gold\", -bioid, -embed, -label, -\"message_gold\", -source, -\"bias:confidence\", -\"_unit_id\", -bias)\n\nRows: 5000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): _unit_state, _last_judgment_at, audience, bias, message, bioid, em...\ndbl  (5): _unit_id, _trusted_judgments, audience:confidence, bias:confidence...\nlgl  (5): _golden, orig__golden, audience_gold, bias_gold, message_gold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata.token6 = data6 %&gt;%\n  unnest_tokens(word, text, token = \"words\")\n\ndata.token_nostop6 = data.token6 %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"http\",\"t.co\",\"amp\",\"day\",\"rt\",\"1\",\"week\",\"watch\",\"morning\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"))\n\ntweet.sentiments6 = data.token_nostop6 %&gt;%\n  inner_join(sent_words, by = \"word\") %&gt;%\n  group_by(id) %&gt;%\n  mutate(sentiment = mean(value)) %&gt;%\n  ungroup() %&gt;%\n  select(-value, -word) %&gt;%\n  distinct()\n\n\nggplot(tweet.sentiments6, aes(x=sentiment, fill=audience)) +\n  geom_density(alpha=.25) +\n  theme_economist_white()\n\n\n\n\nHere, I analyzed the sentiment of entire tweets split by their focus on a politician’s local constituency or the entire nation. Tweets aimed at the entire nation are more negative while tweets aimed at a politician’s local constituency are more positive."
  }
]